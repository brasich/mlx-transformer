{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "\n",
    "def encode(input_string):\n",
    "    return [stoi[char] for char in input_string]\n",
    "\n",
    "def decode(input_list):\n",
    "    return ''.join([itos[i] for i in input_list])\n",
    "\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "block_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(input_dims=n_embd, output_dims=head_size, bias=False)\n",
    "        self.query = nn.Linear(input_dims=n_embd, output_dims=head_size, bias=False)\n",
    "        self.value = nn.Linear(input_dims=n_embd, output_dims=head_size, bias=False)\n",
    "        self.tril = mx.tril(mx.ones((block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        B, T, C = X.shape\n",
    "        k = self.key(X)\n",
    "        q = self.query(X)\n",
    "        wei = q @ k.transpose((0, -1, -2)) * C ** -0.5\n",
    "        wei = mx.where(self.tril[:T, :T] == 0, mx.array(float('-inf')), wei)\n",
    "        wei = nn.softmax(wei, axis=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(X)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = [Head(head_size=head_size) for _ in range(num_heads)]\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        out = mx.concatenate([h(X) for h in self.heads], axis=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def __call__(self, X):\n",
    "        return self.net(X)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        X = X + self.sa(self.ln1(X))\n",
    "        X = X + self.ffwd(self.ln2(X))\n",
    "        return X\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def __call__(self, idx):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(mx.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            idx_next = mx.random.categorical(logits, num_samples=1)\n",
    "            idx = mx.concatenate((idx, idx_next), axis=-1)\n",
    "            print(decode(idx_next[0].tolist()), end='')\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel()\n",
    "model.load_weights('char_level_mpx.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SALANUS:\n",
      "Lord All, thy botch may was ere, thou heaving she upon and negleher and begge heave\n",
      "the havet Clatio though yuse homost, and so excopperoporforce,\n",
      "Be\n",
      "And we to at he pilles\n",
      "deat them old. my have and wais:\n",
      "Him:\n",
      "Vuch thou life, he'rt, ay, one Lord fly a profemmine a creath,\n",
      "that's know meany and by somembers\n",
      "Her leave our life, my heart I crost.\n",
      "\n",
      "First Servant:\n",
      "As the prince that Benister not thing:\n",
      "So he'rt up,\n",
      "Ne's my limped in wither obe with of will persulace.\n",
      "\n",
      "Fince:\n",
      "My Provost, go and cries me nope, sit parish, chang gone\n",
      "With yet I'll eady? not upen ving;\n",
      "But to rembred ance for truphecy well.\n",
      "Beshings, have is wore here\n",
      "Stardertly twells for this neaturm and be night;\n",
      "My all spare weak shall wear I rest offl men\n",
      "We Presently me down, when is not unsterning. I we add.\n",
      "\n",
      "LOUCESTER:\n",
      "Yet peope as it in poors ords I dany,\n",
      "And retiek the bairly heads make livise,\n",
      "To shall chold that prhenclight chamb-seet,\n",
      "So my darker no the warm impree danio,\n",
      "That, my clipy in like of thy in"
     ]
    }
   ],
   "source": [
    "context = mx.zeros((1,1), dtype=mx.int32)\n",
    "generated = model.generate(context, max_new_tokens=1000)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
